1. Overview:

Project Name: Ton Tasks 
Test Plan Version: 1.0
Created By: Sara Rashwan 
Date: 13-9-2024
Sprints: 2 Sprints 
_______________________________
2. Objective:

The objective of this test plan is to ensure that all features developed during Sprint 1 and Sprint 2 meet the acceptance criteria and are free from critical defects.
_______________________________
3. Scope of Testing: 

In-Scope: Functional, integration, and user acceptance testing for features in Sprint 1 and Sprint 2 (login, profile management, payment integration, and report generation).
Out-of-Scope: Features not included in these two sprints (e.g., future features, performance testing, or non-functional requirements that are not planned).
________________________________
4. Test Items:

Sprint 1:
Feature A: User Login
Feature B: User Profile Management

Sprint 2:
Feature C: Payment Integration
Feature D: Report Generation
________________________________
5. Test Strategy:

Test Levels:
A. Unit Testing: Conducted by developers during each sprint to verify individual components.
B. Integration Testing: After each sprint, ensure components interact correctly.
C. System Testing: Test the full application after each sprint to verify functionality end-to-end.
D. User Acceptance Testing (UAT): Performed after Sprint 2 to ensure the system meets user requirements.

Testing Types:
E. Functional Testing: Validate each feature against the requirements.
F. Regression Testing: Ensure previous features work after Sprint 2 developments.
G. Smoke Testing: Verify core functionalities after deployments.
H. Exploratory Testing: Explore the system for unexpected behaviors.
________________________________
6. Test Approach:

Test Design:
Test cases will be designed based on the user stories and acceptance criteria defined for each sprint.

Execution of Testing:
A. Unit Testing
Who: performed by developers.
When: During each sprint, after code is developed but before integration.
How:
Developers will write unit tests using TestNG .
Unit tests will be automated and included in the continuous integration pipeline (e.g., using Bamboo).
Success Criteria: Each unit test must pass, ensuring no defects in isolated code modules.

B. Integration Testing
Who: performed by software integration engineer.
When: After each sprint, once individual components have been unit tested.
How:
The engineer will develop integration test cases to simulate data flow between integrated modules.
These tests are often automated but can include manual testing for complex integrations.
Success Criteria: Integrated components should work together without any unexpected behaviors.

C. System Testing
Who: performed by the QA team.
When: After each sprint, once the integrated system is ready.
How:
System testing involves validating the entire application end-to-end, focusing on functional requirements.
Test cases will cover all features developed in the sprint.
Both automated tests (e.g., using JIRA XRAY) and manual test cases can be used to validate FFFFFFFFFFFFF.
Success Criteria: The entire system should meet all defined requirements and work as expected in its operational environment.

D. User Acceptance Testing (UAT)
Who: Performed by end-users or stakeholders, with support from the QA team.
When: At the end of Sprint 2, once the system is fully developed.
How:
End-users execute predefined test cases based on real-world scenarios and acceptance criteria provided in the project plan.
Performed in a staging environment.
Success Criteria: The system should pass all acceptance criteria, and stakeholders must sign off before the product is released.

E. Functional Testing
Who: Performed by the QA team.
When: After each sprint, during system testing.
How:
Test cases will be based on the acceptance criteria of each user story.
Both manual and automated functional tests are executed to cover different scenarios (e.g., valid inputs, invalid inputs, edge cases).
Success Criteria: Each feature works as intended, with no functional defects.

F. Regression Testing
Who: Performed by the QA team.
When: After Sprint 2, or after any major code changes.
How:
Regression testing ensures that new features introduced in Sprint 2 donâ€™t break or negatively affect features developed in Sprint 1.
done through automated tests, which are added to the continuous integration pipeline and triggered automatically after code changes.
Success Criteria: All previously working features continue to function correctly, without new bugs.

G. Smoke Testing
Who: performed by the QA team.
When: After each new deployment, before deeper testing begins.
How:
quick tests run to verify that the core functionalities of the system are stable and work after deployment.
include checking that the system starts up correctly, users can log in, and essential functions (e.g., FFFFF) are operational.
Smoke tests are typically automated and included in the CI/CD pipeline to ensure rapid feedback after each code merge or deployment.
Success Criteria: The system is stable enough for further testing.

H. Exploratory Testing
Who: Performed by the experienced testers.
When: after system testing and before the final UAT.
How:
unscripted, where testers explore the system without predefined test cases, looking for unexpected behaviors, edge cases, or usability issues.
Success Criteria: No unexpected behaviors are discovered.
________________________________
7. Test Schedule:

Sprint 1:
Test Case Design: Days 1-2
Test Execution: Days 3-5
Defect Reporting: Continuous, until end of sprint

Sprint 2:
Test Case Design: Days 1-2
Test Execution: Days 3-5
Regression Testing: Day 6-7
User Acceptance Testing: Day 8-10
_______________________________
8. Test Environment

Development Environment: For unit and integration testing (e.g., the link for the Dev Environment).
Staging Environment: For system and UAT testing (e.g., the link for the staging Environment).
Tools and frameworks to be used: (e.g.,TestNG for unit testing, Bamboo for CI/CD, Jira for bug tracking, Xray for test management).
_______________________________
9. Test Data:

For Figma project: https://www.figma.com/design/JZ3dDs20PPqnDwIFOhYPHI/ToN-Tasks?node-id=0-1&node-type=canvas&t=tqusoaXWz4KGZNCW-0 .
For Acceptance Criteria : the SRS pdf file.
For Regretion testing : many previous versions of the Application.
________________________________
10. Resources:

List with the people involved in the testing process:

Testers: Sara Rashwan
Developers: [Developer names]
Product Owner: [Name]
________________________________
11. Entry and Exit Criteria:

Entry Criteria:
User stories and acceptance criteria are defined and approved.
Test environments are ready.
Test data is prepared.

Exit Criteria:
All critical and high-severity defects are resolved.
95% of test cases pass.
Stakeholder sign-off for each sprint.
________________________________
12. Risk and Mitigation:

A. Risk: Features may not be fully developed by the end of each sprint.
   Mitigation: Focus on critical paths and conduct parallel testing for already completed features.

B. Risk: Test data may not be available in time.
   Mitigation: Prepare test data well in advance and coordinate with developers.

________________________________
13. Test Deliverables:

Test Plan Document
Test Cases and Test Scripts
Defect Reports
Test Summary Report after each sprint
Final UAT Report
______________________________________________________________________________________________________________________________________________________________________________________________






















